---
title: "Homework Challenge #4[^*]"
subtitle: "ECON 260A"
author: "VillaseÃ±or-Derbez, J.C."
date: "`r Sys.Date()`"
output:
    bookdown::pdf_document2:
        toc: no
        number_sections: false
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
# bibliography: references.bib
---

[^*]: Code for this assignment is available on [GitHub](https://github.com/jcvdav/ECON260A/rmd/Assig4.Rmd)


```{r, echo = F}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      eval = T)

suppressPackageStartupMessages({
  library(startR)
  library(tidyverse)
})
```

# Set up

We consider a two-period investment opportunity with a fixed cost of investment of $I = 400$ and revenues of $V_1 = 200$ for the first year. The next year the investment may generate $V_2 = 600$ with probability $p$ and $V_2 = 100$ with probability $1 - p$. We assume investments are irreversible, and that $V_2$ is only observed until year 2. The discount factor is $\delta = 1$.

## Derive the Dixit-Pindyck option value in termps of $p$

The general payoff on investment would be given by:

$$
v(x_1, x_2, \theta) = u_1(x_1) + u_2(x_1, x_2, \theta)
$$

This shows how $x_1$ affects the period 2 dynamics, because the investment is irriversible. This follows the same notation as the class notes where $x_1$ is the decision to invest or not invest in year 1 and $x_2$ the same for year 2. The only difference is that I define the random shock as $\theta$ instead of $\tilde{\theta}$ for brevity and because there is no need to distinguish this value.

The first year revenues for investing and not investing would be given by:

$$u_1(1) = -400 + 200$$

and

$$u_1(0) = 0$$


For the second year, the function doesn't allow for a choice if we've already invested (*i.e.* $x_1 = 1$), thus:

$$
\begin{split}
u_2(1,1,\theta) = 600p + 100(1 - p) \\
= 500p + 100
\end{split}
$$

However, if we haven't invested we can still chose $x_2$ depending on the then revealed value of $V_2$, thus making:

$$
\max_{x_2 \in\{0, 1\}}\left[u_2(1, x_2, \theta)\right]
$$

The Dixit-Pindyck equation states:

$$
DPOV = \max\{V^l(1), V^l(0)\} - \max\{V^n(1), V^n(0)\}
$$

The superscript $l$ and $n$ denote learning and the naive decision, and $V$ denote the *expected* values given the decissions. The ability to learn does not change the value of investing relative to the naive case, because the decision is irreversible. Therefore $V^l(1) \equiv V^n(1)$ and any of these are given by $V(1)$:

$$
\begin{split}
V(1) &= u_1(1) + \mathrm{E}[u_2(1,1,\theta)] \\
&\text{substituting the above values of }u_1\text{ and } u_2\\
& = -400 + 200 + 600p + 100(1-p) \\
& = -200 + 500p + 100 \\
& = 500p - 100
\end{split}
$$

The value with learning and not investing in the first period is given by:

$$
\begin{split}
V^l(0) &= u_1(0) + \mathrm{E}\left[\max_{x_2 \in\{0, 1\}}\left[u_2(1, x_2, \theta)\right]\right] \\
&= 0 -400 + 600 \\
&= 200
\end{split}
$$


When not investing and with a naive approach, the value is given by:

$$
V^n(0) = 0
$$

Substituting these into the $DPOV$ equation, we obtain:

$$
DPOV = \max\{500p - 100, 200\} - \max\{500p - 100, 0\}
$$

The $DPOV$ captures the valuye of postponing our decision to invest given that we are able to learn about $\theta$. Therefore, the $DPOV \nleq 0$, because $DPOV = 0$ implies that we don't learn, or that what we learn is not valuable enough. The DPOV is shown as a function of P in figure \@ref(fig:dpov).

```{r dpov, fig.height = 3, fig.width = 6, fig.cap = "Dixit-Pindyck Option Value as a function of $p$. Size of the dots indicate the naive value, and colors indicate value with learning."}
tibble(p = seq(0, 1, by = 0.01)) %>% 
  rowwise() %>% 
  mutate(learn = max((500 * p) -100, 200),
         naive = max((500 * p) -100, 0),
         dpov = learn - naive) %>% 
  ggplot(aes(x = p, y = dpov, fill = learn, size = naive)) +
  geom_point(shape = 21, alpha = 0.5) +
  scale_fill_gradientn(colors = colorRamps::matlab.like(50)) +
  guides(fill = guide_colorbar(title = quo(V^l)),
         size = guide_legend(title = quo(V^n))) +
  theme_bw() +
  theme(legend.position = c(0.8, 0.65),
        legend.box = "horizontal",
        legend.background = element_blank(),
        panel.grid = element_blank()) +
  labs(y = "DPV")
```


## Spread in the distribution of year two revenues

Specifically, $V_2 = 600 + 100u$ with probability $p$ and $V_2 = 100 - 100u$ with probability $1 - p$ where $0 \leq u \leq 1$.

We can modify our expected payoffs in the following way:

$$
\begin{split}
V(1) &= u_1(1) + \mathrm{E}[u_2(1,1,\theta)] \\
& = -400 + 200 + (600 + 100u)p + (100-100u)(1-p) \\
& = -400 + 200 + 600p + 100up + 100 -100p -100u +100up \\
& = -100 + 500p + 200up -100
\end{split}
$$

$$
\begin{split}
V^l(0) &= u_1(0) + \mathrm{E}\left[\max_{x_2 \in\{0, 1\}}\left[u_2(1, x_2, \theta)\right]\right] \\
&= 0 -400 + 600 + 100u \\
&= 200 + 100u
\end{split}
$$

$V^n(0)$ remains the same.

Therefore, the $DPOV$ in terms of $p$ and $u$ becomes:

$$
DPOV = \max\{-100 + 500p + 200up -100, 200 + 100u\} - \max\{-100 + 500p + 200up -100, 0\}
$$

Even when u causes an asymetric shock of, increasing values of $u$ cause an increase in $DPOV$. This is because at the second time step, we realize if we'll receive a greater or lower shock (as compared to the previous scenario). In a sense, a larger $u$ just yields a greater $DPOV$ because we know that we'll get an extra $100u$ or that we'll lose an extra $-100u$ and we can avoid these in the same way. Figures \@ref(fig:lines) and \@ref(fig:dpovu) show the relationship between $u$ and $DPOV$ and of $DPOV$ and a spacie between $p$ and $u$..

```{r lines, fig.height = 3, fig.width = 6, fig.cap = "Relationship between DPOV and $u$ for three given values of $p$. Colors indicate the value of $p$, sizes indicate the naive value, and transparency the learning value."}
expand.grid(p = seq(0, 1, by = 0.1),
            u = seq(0, 1, by = 0.1)) %>% 
  rowwise() %>% 
  mutate(learn = max((-100 + (500 * p) + (200 * u * p) -100 * u),
                     (200 + (100 * u))),
         naive = max((-100 + (500 * p) + (200 * u * p) -100 * u),
                     0),
         dpov = learn - naive) %>% 
  ggplot(aes(x = u, y = dpov, color = p, group = p)) +
  geom_line() +
  geom_point(aes(size = naive, alpha = learn), color = "black") +
  scale_color_gradientn(colours = colorRamps::matlab.like(50)) +
  guides(fill = guide_colorbar(title = "DPOV", order = 1),
         alpha = guide_legend(title = quo(V^l)),
         size = guide_legend(title = quo(V^n))) +
  theme_bw() +
  theme(legend.box = "horizontal",
        panel.grid = element_blank()) +
  labs(y = "DPOV")
```


```{r dpovu, fig.height = 3, fig.width = 6, fig.cap = "DPOV under different combinations of $u$ and $p$. Colors indicate DPOV value, dots indicate a subsample (at 0.1 intervals in both axis) that show the values as naive and learning with sizes and transparency, respectively. The dashed vertical white lines show the profiles along which lines of Fig. 2 were extracted."}
expand.grid(p = seq(0, 1, by = 0.1),
            u = seq(0, 1, by = 0.1)) %>% 
  rowwise() %>% 
  mutate(learn = max((-100 + (500 * p) + (200 * u * p) -100 * u),
                     (200 + (100 * u))),
         naive = max((-100 + (500 * p) + (200 * u * p) -100 * u),
                     0),
         dpov = learn - naive) %>% 
  ggplot(aes(x = p, y = u)) +
  geom_raster(aes(fill = dpov), interpolate = T) +
  geom_point(aes(size = naive, alpha = learn)) +
  scale_fill_gradientn(colours = colorRamps::matlab.like(50)) +
  guides(fill = guide_colorbar(title = "DPOV", order = 1),
         alpha = guide_legend(title = quo(V^l)),
         size = guide_legend(title = quo(V^n))) +
  theme_bw() +
  theme(legend.box = "horizontal",
        panel.grid = element_blank()) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  geom_vline(xintercept = seq(0, 1, by = 0.1), linetype = "dashed", color = "white")
```

## Assume $\delta < 1$ and $u = 0$ again.

Since the discount factor enters as a multiplicative term for the second year, we can just propagate it and obtain the following expected payoffs:

$$
\begin{split}
V(1) &= -200 + \delta(500p + 100) \\
V^l(0) &= \delta200 \\
v^n(0) &= 0
\end{split}
$$

$$
DPOV = \max\{-200 + \delta (500p + 100), \delta 200\} - \max\{-200 + \delta (500p + 100), 0\}
$$

The relationship between $\delta$ and $DPOV$ is somewhat similar as the previous case. Here, we see that higher values of $\delta$ produce greater $DPOV$ for the region where $\delta > 0.5$. Very low values of $\delta$ imply that both the good and bad shocks next timestep are worth nothing, and therefore there is no value in learning.

```{r dpovd, fig.height = 3, fig.width = 6, fig.cap = "DPOV under different combinations of $u$ and $p$. Colors indicate DPOV value, dots indicate a subsample (at 0.1 intervals in both axis) that show the values as naive and learning with sizes and transparency, respectively."}
expand.grid(p = seq(0, 1, by = 0.1),
            d = seq(0, 1, by = 0.1)) %>% 
  rowwise() %>% 
  mutate(learn = max((-200 + (d * ((500 * p) + 100))),
                     (200 * d)),
         naive = max((-200 + (d * ((500 * p) + 100))),
                     0),
         dpov = learn - naive) %>% 
  ggplot(aes(x = p, y = d)) +
  geom_raster(aes(fill = dpov), interpolate = T) +
  geom_point(aes(size = naive, alpha = learn)) +
  scale_fill_gradientn(colours = colorRamps::matlab.like(50)) +
  guides(fill = guide_colorbar(title = "DPOV", order = 1),
         alpha = guide_legend(title = quo(V^l)),
         size = guide_legend(title = quo(V^n))) +
  theme_bw() +
  theme(legend.box = "horizontal",
        panel.grid = element_blank()) +
  scale_x_continuous(expand = c(0,0)) +
  scale_y_continuous(expand = c(0,0)) +
  labs(y = quo(delta))
```




















