---
title: "Homework Challenge #1[^*]"
subtitle: "ECON 260A"
author: "VillaseÃ±or-Derbez, J.C."
date: "`r Sys.Date()`"
output:
    bookdown::pdf_document2:
        toc: no
        number_sections: true
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  # - \usepackage{natbib}
# bibliography: references.bib
---

[^*]: Code for this assignment is available on GitHub: [https://github.com/jcvdav/ECON260A](https://github.com/jcvdav/ECON260A)

```{r, echo = F}
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE,
                      eval = T)

suppressPackageStartupMessages({
  library(startR)
  library(tidyverse)
})
```


The stock of a renewable resource has the following equation of motion:

$$
x_{t+1} = f(y_t)
$$

Where $y_t \equiv x_t - h_t$ is the residual stock in period $t$, determined by the difference between stock size at the beginning of that period ($h_t$) and the respective harvest $h_t$. There is no cost of fishing, and there is a price of $p = 1$ such that payoff in period $t$ is simply $h_t$. The discount factor is given by $\delta$.

Since $p = 1$ and we have no costs, payoff $\pi(h_t) = h_t$. Our harvest is not influenced by stock size, therefore the period-$t$ Dynamic Optimization Problem is given by:

$$
\begin{split}
\max_{h_t} \sum_{t = 0}^T\delta^th_t\\
\\
\rm{s.t.}\ x_{t+1} = f(y_t)
\end{split}
$$

# Period-$t$ dynamic programming equation

At every period, our value is determined by our payoff, equivalent to $h_t$, and the discounted value of leaving some resources to harvest in the future (*i.e.* $x_{t+1}$):


$$
V_t(x_t) = \max_{h_t} \left[ h_t + \delta V_{t+1}(x_{t+1}) \right]
$$

Since stock size next period is given by a growth function $f(y_t)$ that depends on what's left from the previous extraction ($x_t - h_t$), then the above becomes:

$$
V_t(x_t) = \max_{h_t} \left[ h_t + \delta V_{t+1}(f(x_t - h_t)) \right]
$$

# Backward induction

## Period-$T$

On the last period, the DPE would be given by:

$$
V_T(x_T) = \max_{h_T} \left[ h_T + \delta V_{T+1}(x_{T+1}) \right]
$$

The discounted term only includes stock size, because there are no harvests in period $T+1$. Since this is a fixed-horizon problem ending at $t = T$ and there is no salvage value, there is no incentive to leave resources for further time periods (*i.e.* $T + 1$), so $V_{T+1}(x_T) = 0$. Therefore, the above becomes:

$$
V_T(x_T) = \max_{h_T} \left[ h_t + 0 \right]
$$

This means that in period $t = T$, it would be optimal to extract whatever is left of the resource ($h^*_T = x_T$), thus making the period-$T$ DPE:

$$V_T(x_T) = x_T$$

## Period-$T-1$

On the previous period, the DPE would be given by:

$$
V_{T-1}(x_{T-1}) =  \max_{h_{T-1}} \left[ h_{T-1} + \delta V_T(x_T) \right]
$$

Since we know that $V_T(x_T) = x_T$, then the above becomes:

$$
V_{T-1}(x_{T-1}) =  \max_{h_{T-1}} \left[ h_{T-1} + \delta x_T \right]
$$

And in order to have stock size $x_T$, it means that the previous period we had stock size $x_{T-1}$ and harvested $h_{T-1}$ which then grew to $x_T$ (*i.e.* $x_T = f(x_{T-1} - h_{T-1})$). Therefore:

$$
V_{T-1}(x_{T-1}) =  \max_{h_{T-1}} \left[ h_{T-1} + \delta f(x_{T-1} - h_{T-1}) \right]
$$
By taking the first derivative, we get that:

$$
f'(x_{T-1}-h_{T-1}) = \frac{1}{\delta}
$$

The intuition is that if I get more returns from harvesting now than from leaving some to grow and harvest it next period, I will keep fishing. Whatever is left after harvest from $T-1$ is the residual stock ($y_t$), in this case denoted by $y^*_{T-1} = x_{T-1} - h^*_{T-1}$. The period-$T-1$ DPE then becomes:

$$
\begin{split}
V_{T-1}(x_{T-1}) &=  h^*_{T-1} + \delta f(y^*_{T-1}) \\
&= x_{T-1} - y^*_{T-1} + \delta f(y^*_{T-1})
\end{split}
$$

This says that in period $T-1$ my value is given by this period's harvest ($h^*_{T-1} = x_{T-1} - y^*_{T-1}$) + the discounted value of leaving something ($y^*_{T-1}$) for the next period. I can re-write the above equation by stating a constant $A \equiv - y^*_{T-1} + \delta f(y^*_{T-1})$ so that it becomes:

$$
V_{T-1}(x_{T-1}) = x_{T-1} + A
$$

## Period-$T-2$

If we go back to the third-to-last period, the DPE is given by:

$$
\begin{split}
V_{T-2}(x_{T-2}) &= \max_{h_{T-2}} \left[ h_{T-2} + \delta V_{T-1}(x_{T-1}) \right]\\
&= \max_{h_{T-2}} \left[ h_{T-2} + \delta (x_{T-1} + A) \right]\\
&= \max_{h_{T-2}} \left[ h_{T-2} + \delta f((x_{T-2}-h_{T-2}) + A) \right]
\end{split}
$$

First order conditions give:

$$
\begin{split}
1 &= \delta f'(x_{T-2}-h_{T-2}) \\
\frac{1}{\delta} &= f'(x_{T-2}-h_{T-2}) \\
1 + r &= f'(x_{T-2}-h_{T-2})
\end{split}
$$

Again, this indicates that I will leave remaining stock $y^*_{T-2}$ after taking $h^*_{T-2}$ from $x_{T-2}$. My DPE becomes:

$$
V_{T-2}(x_{T-2}) = x_{T-2} + B
$$

Up to a constant, where $B = \delta \left[ f(y^*_{T-2} + A) \right] - y^*_{T-2}$, this equation has the same form as the one for period $T-1$. Therefore we can say that our period-$t$ value function is:

$$
V_t(x_t) = x_t + \theta_t
$$

Based on the last two first order conditions taken for the DPE at $T-1$ and $T-2$, it becomes evident that it is optimum for one to harvest down to:

$$\delta f'(x_t - h_t) - 1 = 0$$

# Period-$t$ policy function and $p$

Assuming a linear payoff function, returns are given by the product of a constant price $p$ and my harvest $h_t$:

$$
\pi(h_t) = ph_t
$$

In this case $p$ is just a scaling parameter that should not affect my optimum policy function (provided that *p* is constant through time). Again, we can use backward induction to show this. 

## Period-$T$

At period T, my DPE is:

$$
\begin{split}
V_t(x_t) &= \max_{h_t} \left[ ph_t + \delta V_{t+1}(x_{t+1}) \right] \\
&= \max_{h_t} \left[ ph_t + 0 \right] \\
\end{split}
$$

Which has first order condition of $h^*_T = xT$ leading to a value function with the form $$V_T(x_T) = px_T $$.

## Period-$T-1$

$$
\begin{split}
V_{T-1}(x_{T-1}) &=  \max_{h_{T-1}} \left[ ph_{T-1} + \delta V_T(x_T) \right] \\
&=  \max_{h_{T-1}} \left[ ph_{T-1} + \delta px_T \right]\\
\textrm{factor out } p \\
&=  p\left[\max_{h_{T-1}} \left[ h_{T-1} + \delta x_T \right]\right]\\
&=  p\left[\max_{h_{T-1}} \left[ h_{T-1} + \delta f(x_{T-1} - h_{T-1}) \right]\right]\\
\end{split}
$$

Our first order condition is the same as in the previous exercise. With $p$ being independent of harvest and stock size, it is just a constant so the first order condition is still the same, but multiplied by $p$:

$$
\begin{split}
0 &= 1- \delta f'(y^*_t) \\
\frac{1}{\delta} &= f'(y^*_t)
\end{split}
$$

Yielding a value function of

$$
\begin{split}
V_{T-1}(x_{T-1}) &= ph^*_{T-1} + \delta f'(y^*_{T-1}) \\
&= p(x_{T-1} - y^*_{T-1}) + \delta f'(y^*_{T-1}) \\
&= px_{T-1} + \gamma
\end{split}
$$

Where $\gamma = -py^*_{T-1} + \delta f'(y^*_{T-1})$

## Period-$T-2$

Going back to $T-2$ I should see this hold:

$$
\begin{split}
V_{T-2}(x_{T-2}) &= \max_{h_{T-2}} \left[ ph_{T-2} + \delta V_{T-1}(x_{T-1}) \right] \\
&= \max_{h_{T-2}} \left[ ph_{T-2} + \delta(px_{T-1} + \gamma) \right] \\
\textrm{factor out } p \\
&= p\left[\max_{h_{T-2}} \left[ h_{T-2} + \delta\left(x_{T-1} + \frac{\gamma}{p}\right) \right]\right] \\
&= p\left[\max_{h_{T-2}} \left[ h_{T-2} + \delta\left(f(x_{T-2} - h_{T-2}) + \frac{\gamma}{p}\right) \right]\right] \\
\end{split}
$$

Maximizing the above does not include $p$ (it's only present as a constant $\frac{\gamma}{p}$), which indicates that the policy function is independent of a *constant* price $p$. If price were a function of $h_t$ or $x_t$ this would likely need a different optimization approach.

\clearpage

# Non-linear payoff function

Assuming a period-$t$ payoff function of the form $\pi_t(h_t) = \alpha h_t - \beta h^2_t$ (with $\alpha > 0$ and $\beta > 0$) and *guessing* that the value function is a linear function of the state (*i.e.* $V_t(x_t) = \phi x_t + \theta_t$), my DPE would then become:

$$
\begin{split}
V_t(x_t) &= \max_{h_t} \left[ \pi_t(h_t) + \delta V_{t+1}(x_{t+1}) \right] \\
&= \max_{h_t} \left[ \pi_t(h_t) + \delta(\phi x_t + \theta_{t+1}) \right] \\
&= \max_{h_t} \left[ \pi_t(h_t) + \delta(\phi f(x_t-h_t) + \theta_{t+1}) \right] \\
\end{split}
$$

Which would have first order conditions of:

$$
\begin{split}
\pi_t'(h_t) -\delta \phi f'(x_t-h_t) &= 0 \\
\pi_t'(h_t) &= \delta \phi f'(x_t-h_t) \\
\frac{\alpha + 2\beta h_t}{\delta \phi} &= f'(x_t-h_t) \\
\textrm{At optimal harvest } h^*_t \textrm{ this is:} \\
\frac{\alpha + 2\beta h^*_t}{\delta \phi} &= f'(x_t-h^*_t) \\
\frac{\alpha + 2\beta h^*_t}{\delta \phi} &= f'(y^*_t) \\
\Omega h^*_t &= f'(y^*_t) \\
\end{split}
$$

This form shows that the growth rate of the optimal residual stock $f'(y^*_t)$ is not a constant as before, but that it depends on the optimal level of harvest $h^*t$ multiplied by a constant $\Omega = \frac{\alpha + 2\beta}{\delta \phi}$. In this case, the state is linear in the DPE.

\clearpage

# Numerical solution to stochastic evolution in stock

In a deterministic scenario, the current payoff function allows us derive the policy and value functions with a numerical approach. I first solve a stochastic version of the problem to make sure my code works and so I can draw some intuition about the problem. Figure \ref{fig:deterministic} shows how every time I take a step back (indicated by colors) the policy and value function converge. The policy function takes about 11 steps to visually converge, while the value function takes much more.

Now, this might change once we account for the stochastic growth in our stock ($x_{t+1} = zf(y_t) = zf(x_t - h_t)$), determined by a stochastic term:

$$
z = \begin{cases}
  \textrm{if } p < 0.5 \textrm{ then } 1+\theta \\
  \textrm{if } p > 0.5 \textrm{ then } 1 - \theta
\end{cases}
$$
where $p = B(n =1, p = 0.5)$

My initial thought was that since each state has equal probability of occurring, on average they would cancel out such that the optimal policy function would be the average of the two states. I expected the average of extreme policy functions to be equal to the deterministic path, so I ran three simulations using $z = {0.7, 1, 1.3}$, shown in Figure \ref{fig:stochastic}. This shows how the optimal deterministic policy function is the same as when $z > 1$ for low values of X. On the opposite side, the optimal deterministic policy function is the same as when $z < 1$ for high values of X. This suggests that the timing of positive or negative stochastic events has different influences depending on the state of the resource and the time period in which we are. Furthermore, two or more consecutive negative shocks have a greater marginal effect on $x$ than two or more consecutive positive shocks.

To inspect the relationship between $\theta$, $r$, $\delta$ and $\alpha$ I will do the following:

- Define a space for parameters, where the default value used before is centered
- Hold all other parameters constant at the default values
- Run all possible combinations of Xgrid and the parameter of interest
- Create surface plots and h(x) plots for visual inspection

Figure \ref{fig:sens_z} shows the relationship between $\theta$ (represented by $z$) and the policy function. We see that, all else constant, values of $z > 1$ result in higher levels of harvest for population sizes $X > 15$, while values of $z < 1$ lead to overall lower optimal harvest across a range of $X$. Figure \ref{fig:sens_r} shows the relationship between the intrinsic population growth rate ($r$) and the policy function. The differences are not as observable as before, but it is evident that higher values of $r$ lead to larger harvests, even for smaller stock sizes as compared to low $r$ values. Figure \ref{fig:sens_delta} shows the relationship between $\delta$ and the policy function. Higher discount factors lead to lower optimum harvests across all stock sizes. These differences are particularly large toward small population sizes, where a high discount factor may result in effectively no harvest for small populations. Finally, Figure \ref{fig:sens_a} shows the relationship between $\alpha$ and the policy function. The relationship seems to be quite uniform across a range of small (*i.e.* $X_t < K/2$) stock sizes. However, as the stock grows, larger values of $\alpha$ lead to larger optimum harvests.

```{r}
# Source my objective function
source(here::here("scripts", "hw1", "dof.R"))
# Source a wrapper to call_dof the objective function for multiple experiments
source(here::here("scripts", "hw1", "call_dof.R"))
```


```{r, fig.cap = "\\label{fig:deterministic}Numerical solution of a tereministic case, showing convergence of the policy function and value functions, top and bottom respectively."}
call_dof() %>% 
  gather(variable, value, -c(index, Xgrid, T_end)) %>% 
  ggplot(aes(x = Xgrid, y = value, color = T_end, group = T_end)) +
  geom_line(size = 1) +
  xlab("X") +
  scale_color_gradientn(colours = colorRamps::matlab.like(30)) +
  facet_wrap(~variable, ncol = 1, scales = "free_y") +
  ggtheme_plot()
```



```{r, fig.height = 3, fig.width = 6, fig.cap = "\\label{fig:stochastic}Comparison of running three different scenarios, where z = {0.7, 1, and 1.3} throughout the entire simulation."}
runs <- call_dof(z = 1) %>% 
  mutate(z = 1) %>% 
  rbind(
    call_dof(z = 1.3) %>% 
      mutate(z = 1.3)
  ) %>% 
  rbind(
    call_dof(z = 0.7) %>% 
      mutate(z = 0.7)
  ) %>% 
  filter(T_end == 29)

ggplot(data = runs, mapping = aes(x = Xgrid, y = harvest_opt)) +
  geom_line(size = 1, aes(group = z, color = z)) +
  xlab("X") +
  scale_color_gradientn(colours = colorRamps::matlab.like(30)) +
  ggtheme_plot()
```

```{r}
source(here::here("scripts", "hw1", "sensitivity.R"))
source(here::here("scripts", "hw1", "sens_plot.R"))
```

```{r, fig.cap = "\\label{fig:sens_z}Relationship between z and the policy function. A shows the surface of h* for a combination of X and z. B shows the h(X) with colors for different parameter values."}
sensitivity(min = 0.5, max = 1.5, num = 10, par = "z") %>% 
  sens_plot()
```


```{r, fig.cap = "\\label{fig:sens_r}Relationship between r and the policy function. A shows the surface of h* for a combination of X and r. B shows the h(X) with colors for different parameter values."}
sensitivity(min = 0.1, max = 1, num = 10, par = "r") %>% 
  sens_plot()
```


```{r, fig.cap = "\\label{fig:sens_delta}Relationship between delta and the policy function. A shows the surface of h* for a combination of X and delta. B shows the h(X) with colors for different parameter values."}
sensitivity(min = 0.8, max = 0.99, num = 10, par = "delta") %>% 
  sens_plot()
```

```{r, fig.cap = "\\label{fig:sens_a}Relationship between a and the policy function. A shows the surface of h* for a combination of X and a. B shows the h(X) with colors for different parameter values."}
sensitivity(min = 15, max = 25, num = 10, par = "a") %>% 
  sens_plot()
```


# Project into the future

```{r, fig.cap = "100 simulations for 20 periods. Each simulation is plotted in the background with a different color. The solid dark line represents the mean, and the dashed lines the 1 SD space around the mean. The solid red line represents the optimal path in a deterministic scenario."}
#Grid over state space

#Define some parameters for each call_dof
T_fin <- 20
XL <- 50

delta <- 0.9 #Discount factor
a <- 20 #alpha in payoff function
b <- 0.6 #beta in payoff function
r <- 0.3 #population growth rate
K <- 100 #population carrying capacity
theta <- 0.3 #stochastic part

Xgrid <- seq(from = 0.1,
             to = K,
             length.out = XL)

X0 <- 15 #Initial population size
theta <- 0.3

#Initialize vectors
XX <- vector()
HH <- vector()
Pi <- vector()
Pipv <- vector()


#Starting stock for forward sweep
XX[1] <- X0

#Forward Simulation
simtime <- seq(1, T_fin, length.out=T_fin)

nsims <- 100

results <- data.frame(sim = NA,
                      time = NA,
                      X = NA,
                      H = NA,
                      Profit = NA,
                      PVProfit = NA)

Hstar <- call_dof(returnHs = T)


set.seed(42)
for (sim in 1:nsims){
  
  for (tt in 1:T_fin){
    HHtmp <- spline(Xgrid, Hstar[,tt] , xout = XX[tt] , method = "natural") #Interpolate to find harvest
    
    HH[tt] <- HHtmp$y #Use interpolated harvest in period 
    
    z <- sample(x = c(1 + theta, 1 - theta), size = 1)
    
    XX[tt+1] <- z * (XX[tt]-HH[tt]) + r*(XX[tt]-HH[tt])*(1-(XX[tt]-HH[tt])/K)
    
    # Calculate profits
    Pi[tt] <- a*HH[tt] - b*(HH[tt]^2)
    
    # Discount it
    Pipv[tt] <- (delta^tt)*Pi[tt]
  }
  
  sim_results <- data.frame(sim = sim,
                            time = simtime,
                            X = XX[1:T_fin],
                            H = HH,
                            Profit = Pi,
                            PVProfit = Pipv)
  
  results <- rbind(results, sim_results)
}

for (tt in 1:T_fin){
  HHtmp <- spline(Xgrid, Hstar[,tt] , xout = XX[tt] , method = "natural") #Interpolate to find harvest
  
  HH[tt] <- HHtmp$y #Use interpolated harvest in period 
  
  z <- 1
  
  XX[tt+1] <- z * (XX[tt]-HH[tt]) + r*(XX[tt]-HH[tt])*(1-(XX[tt]-HH[tt])/K)
  
  # Calculate profits
  Pi[tt] <- a*HH[tt] - b*(HH[tt]^2)
  
  # Discount it
  Pipv[tt] <- (delta^tt)*Pi[tt]
}

deter <- data.frame(sim = sim,
                    time = simtime,
                    X = XX[1:T_fin],
                    H = HH,
                    Profit = Pi,
                    PVProfit = Pipv) %>%
  select(sim, time, X, H) %>%
  gather(variable, value, -c(time, sim)) %>% 
  arrange(sim, time, variable)

mean_sd <- function(x){
  data.frame(y = mean(x)) %>% 
    mutate(ymin = y - sd(x),
           ymax = y + sd(x))
}

results %>% 
  select(sim, time, X, H) %>% 
  filter(!is.na(sim)) %>%  
  gather(variable, value, -c(time, sim)) %>% 
  arrange(sim, time, variable) %>% 
  ggplot(data = ., mapping = aes(x = time, y = value)) +
  geom_line(aes(group = sim, color = sim), alpha = 0.5) +
  stat_summary(geom = "ribbon", fun.data = mean_sd, alpha = 0.1, color = "black", linetype = "dashed") +
  stat_summary(geom = "line", fun.y = mean, color = "black", size = 1) +
  geom_line(data = deter, color = "red", size = 1) +
  theme_minimal() +
  scale_color_gradientn(colours = colorRamps::matlab.like(50)) +
  facet_wrap(~variable, scales = "free_y", ncol = 1)
```


























